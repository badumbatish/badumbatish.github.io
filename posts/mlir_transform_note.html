<!DOCTYPE html><!--M7CTb_qAplPo4A_RCn6VQ--><html lang="en" class="px-4 py-4"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/7385e8d9d3c5518f-s.p.ttf" as="font" crossorigin="" type="font/ttf"/><link rel="stylesheet" href="/_next/static/css/3b37cf79ceb9ce8f.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/3864b451a61e4546.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-47bbed6067fe1d45.js"/><script src="/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/_next/static/chunks/964-05defa3e0b8af640.js" async=""></script><script src="/_next/static/chunks/main-app-98bd3314d8e089b7.js" async=""></script><script src="/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/_next/static/chunks/app/posts/%5Bid%5D/page-f0f7551128d4c13f.js" async=""></script><script src="/_next/static/chunks/63-63ed21df6f1fc3e7.js" async=""></script><script src="/_next/static/chunks/app/layout-05e345a4cd50b3e1.js" async=""></script><meta name="next-size-adjust" content=""/><title>[ONGOING] tMLIR Transform note</title><meta name="description" content="Jasmine MLIR transform note"/><link rel="icon" href="/_next/static/media/pfp5.0daa0f7d.jpeg"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d4e0c8 flex flex-col min-h-screen"><div hidden=""><!--$--><!--/$--></div><a href="/"><div class="flex justify-center items-center py-4 "><h1 class="text-4xl font-bold">Jasmine Tang</h1></div></a><div class="flex-1"><div class="py-12 flex flex-col items-center  justify-items-start mx-auto "><a class="flex justify-center text-4xl" href="/blog"><h2>My blog</h2></a><article class="p-8 prose pt-0.5 max-w-none w-full lg:w-1/2 md:w-4/6 sm:w-5/6 prose-sky mx-auto"><div class="flex justify-center text-2xl font-bold "><h2>[ONGOING] tMLIR Transform note</h2></div><div class="flex justify-start text-xl font-bold underline"><h4>2025-07-27</h4></div><div><nav class="toc"><ol class="toc-level toc-level-1"><li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#resources">Resources</a></li><li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#payload-and-transform">Payload and transform</a></li><li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#structured-linalg-ops">Structured linalg ops</a></li><li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#linalggeneric">linalg.generic</a></li><li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#alright-what-about-tiling-and-looping-actually">Alright what about tiling and looping actually?</a></li></ol></nav><h1 id="resources"><a aria-hidden="true" tabindex="-1" href="#resources"><span class="icon icon-link"></span></a>Resources</h1>
<ul>
<li><a href="https://mlir.llvm.org/docs/Tutorials/transform/" rel="nofollow" target="_blank">https://mlir.llvm.org/docs/Tutorials/transform/</a></li>
<li><a href="https://mlir.llvm.org/docs/Dialects/Linalg/" rel="nofollow" target="_blank">https://mlir.llvm.org/docs/Dialects/Linalg/</a></li>
</ul>
<h1 id="payload-and-transform"><a aria-hidden="true" tabindex="-1" href="#payload-and-transform"><span class="icon icon-link"></span></a>Payload and transform</h1>
<p>In the Transform dialect there's two kind of IR:</p>
<ul>
<li>If an IR is operating on others, transforming them, that is called a Transform IR.</li>
<li>If an IR is being operated on, its called a payload IR.</li>
</ul>
<h1 id="structured-linalg-ops"><a aria-hidden="true" tabindex="-1" href="#structured-linalg-ops"><span class="icon icon-link"></span></a>Structured linalg ops</h1>
<p><code>indexing maps</code> and <code>affine_map</code>: a lot of the time, MLIR ops accept something called <code>affine_map</code>, which
is a linear mapping of indexes that an array, a vector or a tensor can iterate on.</p>
<p>iterator_types: now, when you're iterating over these array or vector, you'll need to tell the indexer, for each dimension,
if you're "parallel"-ing (keeping its dimension) or "reduction" (reducing the dimension)</p>
<p>For example, the following code's <code>result</code> use <code>vector.contract</code> op. The operation takes in
two attributes: indexing_maps and iterator_types. There are three affine_map setting in
<code>indexing_maps</code>, as we have 2 inputs and 1 output (making it 3). The singular <code>i</code> is because we only have 1 dimension.
The former <code>i</code> stands for the available index variable to use, and the latter <code>i</code> (if present) stands for the index that the array,
tensor or vector would use.
Since we're producing an integer, the third affine_map goes from <code>(i)</code> to <code>()</code>, thus the <code>reduction</code> type of iterator_types</p>
<p>Note that since we have 8 elements in first input, and 8 elements in second input (same dimension), we can use the same (i).</p>
<pre><code>%init  = arith.constant 0.0 : f32
// Neutral element of multiplication.
%ones = arith.constant dense&#x3C;1.0> : vector&#x3C;8xf32>
// Actual contraction.
%result = vector.contract {
  indexing_maps = [affine_map&#x3C;(i) -> (i)>,
                   affine_map&#x3C;(i) -> (i)>,
                   affine_map&#x3C;(i) -> ()>],
  iterator_types = ["reduction"]
} %0, %ones, %init : vector&#x3C;8xf32>, vector&#x3C;8xf32> into f32
</code></pre>
<p>The pseudo code for the loop is</p>
<pre><code>for i in 0 to 8:
  init += p0[i] * ones[i]
</code></pre>
<p>Alright, seems like you quite got that? Let's do another round:</p>
<pre><code>%result = vector.contract {
  indexing_maps = [affine_map&#x3C;(i, j, k) -> (i, k)>,
                   affine_map&#x3C;(i, j, k) -> (k, j)>,
                   affine_map&#x3C;(i, j, k) -> (i, j)>],
  iterator_types = ["parallel", "parallel", "reduction"]
} %lhs, %rhs, %init: vector&#x3C;8x10xf32>, vector&#x3C;10x16xf32> into vector&#x3C;8x16xf32>
</code></pre>
<p>Here you take 2 input and get out 1 output, so there are 3 affine_maps. Each vector have 3 dimension, so <code>(i, j, k)</code>.
The first vector uses i and k to index its two dimension. The second vector uses k and j to index its two dimension, and the
output vector uses the i and j index. we have 8, 10, 16,so we need to do (i, j, k) instead of just <code>i, j</code>.</p>
<p>Here's the pseudo code in for-loops</p>
<pre><code>for i in 0 to 8:
  for j in 0 to 16:
    for k in 0 to 10:
      init[i, j] += lhs[i, k] * rhs[k, j]
</code></pre>
<h1 id="linalggeneric"><a aria-hidden="true" tabindex="-1" href="#linalggeneric"><span class="icon icon-link"></span></a>linalg.generic</h1>
<p>linalg.generic can be used to implement other operation. Let's do this</p>
<pre><code>linalg.generic {
  indexing_maps [affine_map&#x3C;(i) -> (i)>, affine_map&#x3C;(i) -> (i)>],
  iterator_types = ["parallel"]
} ins(%in : memref&#x3C;?xf32>) outs(%out : memref&#x3C;?xf32>) {
^bb0(%in_one : f32, %out_one : f32):
  %c0 = arith.constant 0.0 : f32
  %0 = arith.cmpf ogt %in_one, %c0 : f32
  %1 = arith.select %0, %in_one, %c0 : f32
  linalg.yield %1 : f32
}
</code></pre>
<p>The prior code shows you how to use <code>linalg.generic</code> to implement a ReLu filter.
The basic block <code>bb0</code> (marked with a ^ in the front) implements the ReLu kernel on a per-index basis,
and the way we index these things are specified by the <code>indexing_maps</code> attribute.
Inside this basic block, you can use other scalar operations to implement the kernel:</p>
<ul>
<li>Initialize 0 as a constant to <code>c0</code>.</li>
<li>Compare the input scalar to <code>c0</code>, and put the result to <code>%0</code>.</li>
<li>Select either the input scalar or <code>c0</code>, depending on the comparison result <code>%0</code>, assigning it to <code>%1</code></li>
<li>Return the <code>%1</code>.</li>
</ul>
<p>Seems easy enough? hahhahahaha</p>
<h1 id="alright-what-about-tiling-and-looping-actually"><a aria-hidden="true" tabindex="-1" href="#alright-what-about-tiling-and-looping-actually"><span class="icon icon-link"></span></a>Alright what about tiling and looping actually?</h1>
<p>Let's talk about tiling first. I first learned about tiling through the PMPP book's example on matrix multiplcation. The transform tutorial couldn't have put it any better:</p>
<blockquote>
<p>Tiling, in general, can be seen as partitioning the iteration space into smaller parts, or tiles, so that the data required by each part fits into a level of cache for example. The order in which tiles are executed must preserve the original data dependencies.</p>
</blockquote>
<pre><code>%0 = scf.forall (%i, %j) in (4, 2)
     shared_outs(%shared = %init) -> (tensor&#x3C;8x16xf32>) {

  // Scale the loop induction variables by the tile sizes.
  %3 = affine.apply affine_map&#x3C;(d0) -> (d0 * 2)>(%i)
  %4 = affine.apply affine_map&#x3C;(d0) -> (d0 * 8)>(%j)

  // Take slices of inputs and outputs. Only the "i" and "j" dimensions are sliced.
  %lhs_slice = tensor.extract_slice %lhs[%3, 0] [2, 10] [1, 1]
             : tensor&#x3C;8x10xf32> to tensor&#x3C;2x10xf32>
  %rhs_slice = tensor.extract_slice %rhs[0, %4] [10, 8] [1, 1]
             : tensor&#x3C;10x16xf32> to tensor&#x3C;10x8xf32>
  %result_slice = tensor.extract_slice %shared[%3, %4] [2, 8] [1, 1]
                : tensor&#x3C;8x16xf32> to tensor&#x3C;2x8xf32>

  // This is exactly the same operation as before, but now operating on smaller
  // slices of data.
  %partial =  linalg.generic {
  indexing_maps = [affine_map&#x3C;(i, j, k) -> (i, k)>,
                   affine_map&#x3C;(i, j, k) -> (k, j)>,
                   affine_map&#x3C;(i, j, k) -> (i, j)>],
  iterator_types = ["parallel", "parallel", "reduction"]
  } ins(%lhs_slice, %rhs_slice : tensor&#x3C;2x10xf32>, tensor&#x3C;10x8xf32>)
    outs(%result_slice : tensor&#x3C;2x8xf32>) -> tensor&#x3C;2x8xf32> {
  ^bb0(%lhs_one: f32, %rhs_one: f32, %init_one: f32):
    %0 = arith.mulf %lhs_one, %rhs_one : f32
    %1 = arith.addf %init_one, %0 : f32
    linalg.yield %1 : f32
  } : tensor&#x3C;2x8xf32>

  // Terminator for the loop with tensor-insertion semantics. Inserts a slice
  // into a larger tensor, potentially in parallel.
  scf.forall.in_parallel {
    tensor.parallel_insert_slice %partial into %shared[%3, %4] [2, 8] [1, 1]
        : tensor&#x3C;2x8xf32> into tensor&#x3C;8x16xf32>
  }
}
</code></pre>
<p>scf.forall evaluates a block multiple time in parallel.</p>
<p>A tile in the transform IR is a slice of the original data:</p>
<blockquote>
<p>In the case of linalg.generic operations, the iteration space is implicit and is defined by the shape of the operands. Therefore, a tile can be expressed by performing the same operation on a subset (slice) of the original data</p>
</blockquote>
<p>Arghhhh i hate this i don't get thissssss arghhhhhhhhhhh what the hell is going onnnnnnnnnnn.</p>
<p>gonna go watch netflix and ice cream and takoyaki</p></div></article></div><!--$--><!--/$--></div><button class="bottom-4 right-4 fixed"><div class="inline text-2xl font-bold border-black border-2 rounded  p-0 m-0 h-100 w-100">⇡⇡</div></button><footer class="footer self-center justify-center gap-2 pt-4 items-center italic "><p>Built by Jasmine with NextJS, TailwindCSS, and a tonnn of loveee :)</p></footer><script src="/_next/static/chunks/webpack-47bbed6067fe1d45.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"880\",\"static/chunks/app/posts/%5Bid%5D/page-f0f7551128d4c13f.js\"],\"\"]\n3:I[7555,[],\"\"]\n4:I[1295,[],\"\"]\n5:I[7921,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"63\",\"static/chunks/63-63ed21df6f1fc3e7.js\",\"177\",\"static/chunks/app/layout-05e345a4cd50b3e1.js\"],\"default\"]\n7:I[9665,[],\"OutletBoundary\"]\n9:I[4911,[],\"AsyncMetadataOutlet\"]\nb:I[9665,[],\"ViewportBoundary\"]\nd:I[9665,[],\"MetadataBoundary\"]\ne:\"$Sreact.suspense\"\n10:I[8393,[],\"\"]\n:HL[\"/_next/static/media/7385e8d9d3c5518f-s.p.ttf\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/ttf\"}]\n:HL[\"/_next/static/css/3b37cf79ceb9ce8f.css\",\"style\"]\n:HL[\"/_next/static/css/3864b451a61e4546.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"M7CTb-qAplPo4A_RCn6VQ\",\"p\":\"\",\"c\":[\"\",\"posts\",\"mlir_transform_note\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"id\",\"mlir_transform_note\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/3b37cf79ceb9ce8f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"px-4 py-4\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_d4e0c8 flex flex-col min-h-screen\",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center py-4 \",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold\",\"children\":\"Jasmine Tang\"}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L5\",null,{}],[\"$\",\"footer\",null,{\"className\":\"footer self-center justify-center gap-2 pt-4 items-center italic \",\"children\":[\"$\",\"p\",null,{\"children\":\"Built by Jasmine with NextJS, TailwindCSS, and a tonnn of loveee :)\"}]}]]}]}]]}],{\"children\":[\"posts\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"mlir_transform_note\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/3864b451a61e4546.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":\"$Lf\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"11:T22e2,"])</script><script>self.__next_f.push([1,"\u003cnav class=\"toc\"\u003e\u003col class=\"toc-level toc-level-1\"\u003e\u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#resources\"\u003eResources\u003c/a\u003e\u003c/li\u003e\u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#payload-and-transform\"\u003ePayload and transform\u003c/a\u003e\u003c/li\u003e\u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#structured-linalg-ops\"\u003eStructured linalg ops\u003c/a\u003e\u003c/li\u003e\u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#linalggeneric\"\u003elinalg.generic\u003c/a\u003e\u003c/li\u003e\u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#alright-what-about-tiling-and-looping-actually\"\u003eAlright what about tiling and looping actually?\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/nav\u003e\u003ch1 id=\"resources\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#resources\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResources\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://mlir.llvm.org/docs/Tutorials/transform/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://mlir.llvm.org/docs/Tutorials/transform/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://mlir.llvm.org/docs/Dialects/Linalg/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://mlir.llvm.org/docs/Dialects/Linalg/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"payload-and-transform\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#payload-and-transform\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePayload and transform\u003c/h1\u003e\n\u003cp\u003eIn the Transform dialect there's two kind of IR:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf an IR is operating on others, transforming them, that is called a Transform IR.\u003c/li\u003e\n\u003cli\u003eIf an IR is being operated on, its called a payload IR.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"structured-linalg-ops\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#structured-linalg-ops\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStructured linalg ops\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003eindexing maps\u003c/code\u003e and \u003ccode\u003eaffine_map\u003c/code\u003e: a lot of the time, MLIR ops accept something called \u003ccode\u003eaffine_map\u003c/code\u003e, which\nis a linear mapping of indexes that an array, a vector or a tensor can iterate on.\u003c/p\u003e\n\u003cp\u003eiterator_types: now, when you're iterating over these array or vector, you'll need to tell the indexer, for each dimension,\nif you're \"parallel\"-ing (keeping its dimension) or \"reduction\" (reducing the dimension)\u003c/p\u003e\n\u003cp\u003eFor example, the following code's \u003ccode\u003eresult\u003c/code\u003e use \u003ccode\u003evector.contract\u003c/code\u003e op. The operation takes in\ntwo attributes: indexing_maps and iterator_types. There are three affine_map setting in\n\u003ccode\u003eindexing_maps\u003c/code\u003e, as we have 2 inputs and 1 output (making it 3). The singular \u003ccode\u003ei\u003c/code\u003e is because we only have 1 dimension.\nThe former \u003ccode\u003ei\u003c/code\u003e stands for the available index variable to use, and the latter \u003ccode\u003ei\u003c/code\u003e (if present) stands for the index that the array,\ntensor or vector would use.\nSince we're producing an integer, the third affine_map goes from \u003ccode\u003e(i)\u003c/code\u003e to \u003ccode\u003e()\u003c/code\u003e, thus the \u003ccode\u003ereduction\u003c/code\u003e type of iterator_types\u003c/p\u003e\n\u003cp\u003eNote that since we have 8 elements in first input, and 8 elements in second input (same dimension), we can use the same (i).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e%init  = arith.constant 0.0 : f32\n// Neutral element of multiplication.\n%ones = arith.constant dense\u0026#x3C;1.0\u003e : vector\u0026#x3C;8xf32\u003e\n// Actual contraction.\n%result = vector.contract {\n  indexing_maps = [affine_map\u0026#x3C;(i) -\u003e (i)\u003e,\n                   affine_map\u0026#x3C;(i) -\u003e (i)\u003e,\n                   affine_map\u0026#x3C;(i) -\u003e ()\u003e],\n  iterator_types = [\"reduction\"]\n} %0, %ones, %init : vector\u0026#x3C;8xf32\u003e, vector\u0026#x3C;8xf32\u003e into f32\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe pseudo code for the loop is\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efor i in 0 to 8:\n  init += p0[i] * ones[i]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlright, seems like you quite got that? Let's do another round:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e%result = vector.contract {\n  indexing_maps = [affine_map\u0026#x3C;(i, j, k) -\u003e (i, k)\u003e,\n                   affine_map\u0026#x3C;(i, j, k) -\u003e (k, j)\u003e,\n                   affine_map\u0026#x3C;(i, j, k) -\u003e (i, j)\u003e],\n  iterator_types = [\"parallel\", \"parallel\", \"reduction\"]\n} %lhs, %rhs, %init: vector\u0026#x3C;8x10xf32\u003e, vector\u0026#x3C;10x16xf32\u003e into vector\u0026#x3C;8x16xf32\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere you take 2 input and get out 1 output, so there are 3 affine_maps. Each vector have 3 dimension, so \u003ccode\u003e(i, j, k)\u003c/code\u003e.\nThe first vector uses i and k to index its two dimension. The second vector uses k and j to index its two dimension, and the\noutput vector uses the i and j index. we have 8, 10, 16,so we need to do (i, j, k) instead of just \u003ccode\u003ei, j\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eHere's the pseudo code in for-loops\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efor i in 0 to 8:\n  for j in 0 to 16:\n    for k in 0 to 10:\n      init[i, j] += lhs[i, k] * rhs[k, j]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"linalggeneric\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#linalggeneric\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003elinalg.generic\u003c/h1\u003e\n\u003cp\u003elinalg.generic can be used to implement other operation. Let's do this\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elinalg.generic {\n  indexing_maps [affine_map\u0026#x3C;(i) -\u003e (i)\u003e, affine_map\u0026#x3C;(i) -\u003e (i)\u003e],\n  iterator_types = [\"parallel\"]\n} ins(%in : memref\u0026#x3C;?xf32\u003e) outs(%out : memref\u0026#x3C;?xf32\u003e) {\n^bb0(%in_one : f32, %out_one : f32):\n  %c0 = arith.constant 0.0 : f32\n  %0 = arith.cmpf ogt %in_one, %c0 : f32\n  %1 = arith.select %0, %in_one, %c0 : f32\n  linalg.yield %1 : f32\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe prior code shows you how to use \u003ccode\u003elinalg.generic\u003c/code\u003e to implement a ReLu filter.\nThe basic block \u003ccode\u003ebb0\u003c/code\u003e (marked with a ^ in the front) implements the ReLu kernel on a per-index basis,\nand the way we index these things are specified by the \u003ccode\u003eindexing_maps\u003c/code\u003e attribute.\nInside this basic block, you can use other scalar operations to implement the kernel:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitialize 0 as a constant to \u003ccode\u003ec0\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eCompare the input scalar to \u003ccode\u003ec0\u003c/code\u003e, and put the result to \u003ccode\u003e%0\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eSelect either the input scalar or \u003ccode\u003ec0\u003c/code\u003e, depending on the comparison result \u003ccode\u003e%0\u003c/code\u003e, assigning it to \u003ccode\u003e%1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eReturn the \u003ccode\u003e%1\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSeems easy enough? hahhahahaha\u003c/p\u003e\n\u003ch1 id=\"alright-what-about-tiling-and-looping-actually\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#alright-what-about-tiling-and-looping-actually\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlright what about tiling and looping actually?\u003c/h1\u003e\n\u003cp\u003eLet's talk about tiling first. I first learned about tiling through the PMPP book's example on matrix multiplcation. The transform tutorial couldn't have put it any better:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTiling, in general, can be seen as partitioning the iteration space into smaller parts, or tiles, so that the data required by each part fits into a level of cache for example. The order in which tiles are executed must preserve the original data dependencies.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cpre\u003e\u003ccode\u003e%0 = scf.forall (%i, %j) in (4, 2)\n     shared_outs(%shared = %init) -\u003e (tensor\u0026#x3C;8x16xf32\u003e) {\n\n  // Scale the loop induction variables by the tile sizes.\n  %3 = affine.apply affine_map\u0026#x3C;(d0) -\u003e (d0 * 2)\u003e(%i)\n  %4 = affine.apply affine_map\u0026#x3C;(d0) -\u003e (d0 * 8)\u003e(%j)\n\n  // Take slices of inputs and outputs. Only the \"i\" and \"j\" dimensions are sliced.\n  %lhs_slice = tensor.extract_slice %lhs[%3, 0] [2, 10] [1, 1]\n             : tensor\u0026#x3C;8x10xf32\u003e to tensor\u0026#x3C;2x10xf32\u003e\n  %rhs_slice = tensor.extract_slice %rhs[0, %4] [10, 8] [1, 1]\n             : tensor\u0026#x3C;10x16xf32\u003e to tensor\u0026#x3C;10x8xf32\u003e\n  %result_slice = tensor.extract_slice %shared[%3, %4] [2, 8] [1, 1]\n                : tensor\u0026#x3C;8x16xf32\u003e to tensor\u0026#x3C;2x8xf32\u003e\n\n  // This is exactly the same operation as before, but now operating on smaller\n  // slices of data.\n  %partial =  linalg.generic {\n  indexing_maps = [affine_map\u0026#x3C;(i, j, k) -\u003e (i, k)\u003e,\n                   affine_map\u0026#x3C;(i, j, k) -\u003e (k, j)\u003e,\n                   affine_map\u0026#x3C;(i, j, k) -\u003e (i, j)\u003e],\n  iterator_types = [\"parallel\", \"parallel\", \"reduction\"]\n  } ins(%lhs_slice, %rhs_slice : tensor\u0026#x3C;2x10xf32\u003e, tensor\u0026#x3C;10x8xf32\u003e)\n    outs(%result_slice : tensor\u0026#x3C;2x8xf32\u003e) -\u003e tensor\u0026#x3C;2x8xf32\u003e {\n  ^bb0(%lhs_one: f32, %rhs_one: f32, %init_one: f32):\n    %0 = arith.mulf %lhs_one, %rhs_one : f32\n    %1 = arith.addf %init_one, %0 : f32\n    linalg.yield %1 : f32\n  } : tensor\u0026#x3C;2x8xf32\u003e\n\n  // Terminator for the loop with tensor-insertion semantics. Inserts a slice\n  // into a larger tensor, potentially in parallel.\n  scf.forall.in_parallel {\n    tensor.parallel_insert_slice %partial into %shared[%3, %4] [2, 8] [1, 1]\n        : tensor\u0026#x3C;2x8xf32\u003e into tensor\u0026#x3C;8x16xf32\u003e\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003escf.forall evaluates a block multiple time in parallel.\u003c/p\u003e\n\u003cp\u003eA tile in the transform IR is a slice of the original data:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn the case of linalg.generic operations, the iteration space is implicit and is defined by the shape of the operands. Therefore, a tile can be expressed by performing the same operation on a subset (slice) of the original data\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eArghhhh i hate this i don't get thissssss arghhhhhhhhhhh what the hell is going onnnnnnnnnnn.\u003c/p\u003e\n\u003cp\u003egonna go watch netflix and ice cream and takoyaki\u003c/p\u003e"])</script><script>self.__next_f.push([1,"6:[\"$\",\"div\",null,{\"className\":\"py-12 flex flex-col items-center  justify-items-start mx-auto \",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/blog\",\"className\":\"flex justify-center text-4xl\",\"children\":[\"$\",\"h2\",null,{\"children\":\"My blog\"}]}],[\"$\",\"article\",null,{\"className\":\"p-8 prose pt-0.5 max-w-none w-full lg:w-1/2 md:w-4/6 sm:w-5/6 prose-sky mx-auto\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-center text-2xl font-bold \",\"children\":[\"$\",\"h2\",null,{\"children\":\"[ONGOING] tMLIR Transform note\"}]}],[\"$\",\"div\",null,{\"className\":\"flex justify-start text-xl font-bold underline\",\"children\":[\"$\",\"h4\",null,{\"children\":\"2025-07-27\"}]}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$11\"}}]]}]]}]\n"])</script><script>self.__next_f.push([1,"12:I[8175,[],\"IconMark\"]\na:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"[ONGOING] tMLIR Transform note\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Jasmine MLIR transform note\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/_next/static/media/pfp5.0daa0f7d.jpeg\"}],[\"$\",\"$L12\",\"3\",{}]],\"error\":null,\"digest\":\"$undefined\"}\nf:\"$a:metadata\"\n"])</script></body></html>